{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Generation using GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TF without GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('Running TF without GPU')\n",
    "else:\n",
    "    print(f'Found GPU at {device_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import math\n",
    "import os\n",
    "import seaborn as sn\n",
    "import time\n",
    "\n",
    "from abc import abstractstaticmethod\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "\n",
    "sn.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: target 'output/gan//old/' is not a directory\r\n"
     ]
    }
   ],
   "source": [
    "NOISE_DIMENSION = 128\n",
    "TRAIN_SIZE = 60000\n",
    "TEST_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "N_EXAMPLES = 25\n",
    "G_LEARNING_RATE = 2e-4\n",
    "D_LEARNING_RATE = 1e-4\n",
    "EPSILON = 1e-7\n",
    "\n",
    "ARCH = 'gan'\n",
    "METRICS_PATH = f'metrics/{ARCH}/'\n",
    "OUTPUT_PATH = f'output/{ARCH}/'\n",
    "\n",
    "!mkdir -p $METRICS_PATH\n",
    "!mkdir -p $OUTPUT_PATH\n",
    "!mkdir -p $(OUTPUT_PATH)old/\n",
    "!mv $(OUTPUT_PATH)* $(OUTPUT_PATH)old/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert NOISE_DIMENSION > 0\n",
    "assert TRAIN_SIZE <= 600000\n",
    "assert BATCH_SIZE >= 1\n",
    "assert EPOCHS >= 1\n",
    "assert N_EXAMPLES >= 1\n",
    "assert G_LEARNING_RATE > 0\n",
    "assert D_LEARNING_RATE > 0\n",
    "assert EPSILON > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNetwork(tf.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, input_data, training=False) -> tf.Tensor:\n",
    "        output_data = input_data\n",
    "        for layer in self.layers:\n",
    "            output_data = layer(output_data, training=training)\n",
    "        return output_data\n",
    "\n",
    "    @abstractstaticmethod\n",
    "    def loss() -> tf.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractstaticmethod\n",
    "    def optimizer(*args, **kwargs) -> tf.optimizers.Optimizer:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Network\n",
    "### Layers\n",
    "<pre>\n",
    "<b>Input</b>:  input_size=(128, 1)\n",
    "<b>Dense</b>:  units=128, activation=ReLU\n",
    "<b>Dense</b>:  units=128, activation=ReLU\n",
    "<b>Dense</b>:  units=256, activation=ReLU\n",
    "<b>Dense</b>:  units=512, activation=ReLU\n",
    "<b>Dense</b>:  units=512, activation=ReLU\n",
    "<b>Dense</b>:  units=784, activation=tanh, target_shape=(28, 28, 1)\n",
    "</pre>\n",
    "\n",
    "### Optimizer\n",
    "<pre>\n",
    "<b>Adam</b>:  learning_rate=0.0002\n",
    "</pre>\n",
    "\n",
    "### Loss\n",
    "&#8466;<sub>G</sub>(<i><b>z</b></i>) = <sup>-1</sup>&frasl;<sub>m</sub> &lowast; &sum;<sub><i>i</i></sub> log(<i>D</i>(G(<i><b>z</b><sup>(i)</sup></i>)))\n",
    "\n",
    "### Goal\n",
    "Find argmin<sub>G</sub> {&#8466;<sub>G</sub>(<i><b>z</b></i>)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(BaseNetwork):\n",
    "    def __init__(self, noise_dimension: int, output_shape: tuple) -> None:\n",
    "        super().__init__()\n",
    "        # Network layers\n",
    "        self.layers = [\n",
    "            keras.layers.InputLayer(input_shape=(noise_dimension,)),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(units=128),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=128),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=256),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=512),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=512),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=tf.reduce_prod(output_shape)),\n",
    "            keras.layers.Reshape(target_shape=output_shape),\n",
    "            keras.layers.Activation(tf.nn.tanh),\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def optimizer(learning_rate: float, momentum: float=0.0) -> tf.optimizers.Optimizer:\n",
    "        return tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    @tf.function\n",
    "    def loss(generated_output):\n",
    "        loss_i = tf.math.log(generated_output+EPSILON)\n",
    "        loss = -tf.math.reduce_mean(loss_i)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network\n",
    "### Layers\n",
    "<pre>\n",
    "<b>Input</b>:    input_size=(28, 28, 1)\n",
    "<b>Dense</b>:    units=512, activation=ReLU\n",
    "<b>Dropout</b>:  rate=0.4\n",
    "<b>Dense</b>:    units=512, activation=ReLU\n",
    "<b>Dropout</b>:  rate=0.3\n",
    "<b>Dense</b>:    units=512, activation=ReLU\n",
    "<b>Dropout</b>:  rate=0.3\n",
    "<b>Dense</b>:    units=256, activation=ReLU\n",
    "<b>Dropout</b>:  rate=0.3\n",
    "<b>Dense</b>:    units=128, activation=ReLU\n",
    "<b>Dropout</b>:  rate=0.1\n",
    "<b>Dense</b>:    units=64, activation=ReLU\n",
    "<b>Dense</b>:    units=1, activation=sigmoid\n",
    "</pre>\n",
    "\n",
    "### Optimizer\n",
    "<pre>\n",
    "<b>Adam</b>:         learning_rate=0.0002\n",
    "</pre>\n",
    "\n",
    "### Loss\n",
    "&#8466;<sub>D</sub>(<i><b>x</b>,<b>z</b></i>) = <sup>-1</sup>&frasl;<sub>m</sub> &lowast; &sum;<sub><i>i</i></sub> \\[log <i>D</i>(<i><b>x</b><sup>(i)</sup></i>) + log (1-<i>D</i>(G(<i><b>z</b><sup>(i)</sup></i>)))\\]\n",
    "\n",
    "### Goal\n",
    "Find argmin<sub>D</sub> &#8466;<sub>D</sub>(<i><b>x</b></i>,<i><b>z</b></i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(BaseNetwork):\n",
    "    def __init__(self, input_shape: tuple) -> None:\n",
    "        super().__init__()\n",
    "        # Network layers\n",
    "        self.layers = [\n",
    "            keras.layers.InputLayer(input_shape=input_shape),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(units=512),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dropout(rate=0.4),\n",
    "            keras.layers.Dense(units=512),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dropout(rate=0.3),\n",
    "            keras.layers.Dense(units=256),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dropout(rate=0.3),\n",
    "            keras.layers.Dense(units=128),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dropout(rate=0.2),\n",
    "            keras.layers.Dense(units=64),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=1),\n",
    "            keras.layers.Activation(tf.nn.sigmoid),\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def optimizer(learning_rate: float, momentum: float=0.0):\n",
    "        return tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    @tf.function\n",
    "    def loss(trained_ouput, generated_output) -> tf.Tensor:\n",
    "        loss_i = tf.math.log(trained_ouput+EPSILON) + tf.math.log1p(-generated_output+EPSILON)\n",
    "        loss = -tf.math.reduce_mean(loss_i)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* Import MNIST training images\n",
    "* Normalize to \\[-1, 1\\]\n",
    "* Shuffle and batch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, _), (test_images, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = tf.dtypes.cast((train_images[:TRAIN_SIZE]-127.5) / 127.5, tf.float32)\n",
    "train_images = tf.expand_dims(input=train_images, axis=-1)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_images) \\\n",
    "                          .shuffle(TRAIN_SIZE) \\\n",
    "                          .batch(BATCH_SIZE)\n",
    "\n",
    "test_images = tf.dtypes.cast((train_images[:TEST_SIZE]-127.5) / 127.5, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(noise_dimension=NOISE_DIMENSION, output_shape=(train_images.shape[1:]))\n",
    "generator_optimizer = generator.optimizer(G_LEARNING_RATE)\n",
    "\n",
    "discriminator = Discriminator(input_shape=(train_images.shape[1:]))\n",
    "discriminator_optimizer = discriminator.optimizer(D_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = tf.random.normal([N_EXAMPLES, NOISE_DIMENSION])\n",
    "metrics_names = ['g_loss', 'd_loss', 'acc', 'real_acc', 'fake_acc']\n",
    "batch_history = { name: [] for name in metrics_names }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_sample(generator: Generator, epoch: int, save: bool=True, show=True):\n",
    "    fixed_predictions = generator(fixed_noise)\n",
    "    random_predictions = generator(tf.random.normal(fixed_noise.shape))\n",
    "\n",
    "    f = plt.figure(figsize=(8, 4))\n",
    "    f.suptitle('GAN Output')\n",
    "    outer = gridspec.GridSpec(1, 2)\n",
    "    titles = ['Fixed Samples', 'Random Samples']\n",
    "    data = [fixed_predictions, random_predictions]\n",
    "    for i in range(2):\n",
    "        inner = gridspec.GridSpecFromSubplotSpec(int(math.sqrt(N_EXAMPLES)), int(math.sqrt(N_EXAMPLES)), subplot_spec=outer[i])\n",
    "        predictions = data[i]\n",
    "        for j in range(predictions.shape[0]):\n",
    "            ax = plt.Subplot(f, inner[j])\n",
    "            ax.imshow(predictions[j]*127.5 + 127.5, cmap=plt.cm.gray)\n",
    "            ax.axis('off')\n",
    "            if j == int(math.sqrt(N_EXAMPLES)) // 2:\n",
    "                ax.set_title(titles[i])\n",
    "            f.add_subplot(ax)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(OUTPUT_PATH, 'epoch_{:04d}.png'.format(epoch)))\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_accuracy(real_output) -> tf.Tensor:\n",
    "    # Trained images fed into D have output 1 \n",
    "    correct_real_output = tf.ones_like(real_output)\n",
    "    real_acc = tf.math.reduce_mean(tf.dtypes.cast(tf.math.equal(tf.math.round(real_output), correct_real_output), tf.float32))\n",
    "    return real_acc\n",
    "\n",
    "def accuracy(real_output, fake_output) -> tuple:\n",
    "    # Images from G's noisy distribution should have output 0 from D\n",
    "    correct_fake_output = tf.zeros_like(fake_output)\n",
    "    fake_acc = tf.math.reduce_mean(tf.dtypes.cast(tf.math.equal(tf.math.round(fake_output), correct_fake_output), tf.float32))\n",
    "\n",
    "    return real_accuracy(real_output), fake_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Algorithm\n",
    "<pre>\n",
    "<b>for</b> number of training iterations <b>do</b>\n",
    "  <b>for</b> k steps <b>do</b>\n",
    "     • Sample minibatch of <i>m</i> noise samples {<i><b>z</b><sup>(1)</sup>, ..., <b>z</b><sup>(m)</sup></i>} from noise prior <i>p<sub>g</sub>(<b>z</b>)</i>.\n",
    "     • Sample minibatch of <i>m</i> examples {<i><b>x</b><sup>(1)</sup>, ..., <b>x</b><sup>(m)</sup></i>} from data generating distribution <i>p<sub>data</sub>(<b>x</b>)</i>.\n",
    "     • Update the discriminator by <u>ascending</u> its stochastic gradient:\n",
    "       <center>&Del;<sub>&theta;<sub>d</sub></sub> <sup>1</sup>&frasl;<sub>m</sub> &lowast; &sum;<sub><i>i</i></sub> [log <i>D</i>(<i><b>x</b><sup>(i)</sup></i>) + log(1-<i>D</i>(G(<i><b>z</b><sup>(i)</sup></i>)))]</center>\n",
    "  <b>end for</b>\n",
    "  • Sample minibatch of <i>m</i> noise samples {<i><b>z</b><sup>(1)</sup>, ..., <b>z</b><sup>(m)</sup></i>} from noise prior <i>p<sub>g</sub>(<b>z</b>)</i>.\n",
    "  • Update the generator by <u>descending</u> its stochastic gradient:\n",
    "  <center>&Del;<sub>&theta;<sub>d</sub></sub> <sup>1</sup>&frasl;<sub>m</sub> &lowast; &sum;<sub><i>i</i></sub> log(1-<i>D</i>(G(<i><b>z</b><sup>(i)</sup></i>)))</center>\n",
    "<b>end for</b>\n",
    "</pre>\n",
    "\n",
    "Source: https://arxiv.org/pdf/1406.2661.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1/100\n",
      "1875/1875 [==============================] - 196s 105ms/step - g_loss: 1.0127 - d_loss: 1.2065 - acc: 0.6094 - real_acc: 0.4688 - fake_acc: 0.75001:27 - g_loss: 0.9521 - d_loss: 1.2010 - acc: 0.6406 - real_acc: 0.718 - ETA: 1:26 - g_loss: 1.4233 - d_loss: 0.6600 - acc: 0.8281 - real_acc: 0.8125 - fake_acc: 0.84 - ETA: 1:26 - g_loss: 1.1310 - d_loss: 0.8813 - acc: 0.8125 - real_acc: 0.8750 - fake_acc: 0. - ETA: 1:26 - g_loss: 1.0603 - d_loss: 0.8881 - acc: 0.7969 - real_acc: 0.8750 - fake_acc: 0.71 - ETA: 1:26 - g_loss: 1.1087 - d_loss: 0.8674 - acc: 0.8125 - real_acc: 0.9375 - fake_acc: 0.68 - ETA: 1:25 - g_loss: 0.8786 - d_loss: 1.0319 - acc: 0.7344 - real_acc: 0.9062 - fake_acc: 0. - ETA: 1:25 - g_loss: 1.5641 - d_loss: 0.6749 - acc: 0.8750 - real_acc: 0.8438 - fake_acc: 0. - ETA: 1:25 - g_loss: 1.6145 - d_loss: 0.9115 - acc: 0.7812 - real_acc: 0.6875 - fake_acc: 0.87 - ETA: 1:25 - g_loss: 1.4400 - d_loss: 0.9178 - - ETA: 1:23 - g_loss: 0.9888 - d_loss: 1.4036 - acc: 0.5938 - real_acc: 0 - ETA: 1:22 - g_loss: 1.2365 - d_loss: 0.8624 - acc: 0.7969 - real_acc: 0.7500 - fake_acc: 0.84 - ETA: 1:22 - g_loss: 1.3491 - d_loss: 0.8512 - acc: 0.7500 - real_acc: 0.7188 - fake_acc: 0. - ETA: 1:21 - g_loss: 1.2467 - d_loss: 0.6695 - acc: 0.8750 - real_acc: 0.8750 - fake_a - ETA: 1:21 - g_loss: 1.4023 - d_loss: 0.6899 - acc: 0.8594 - real_acc: 0.9688 - fake_acc: 0.75 - ETA: 1:21 - g_loss: 1.5425 - d_loss: 0.5984 - acc: 0.9062 - real_acc: 0.9062 - fake_acc:  - ETA: 1:21 - g_loss: 1.2980 - d_loss: 0.8895 - acc: 0.8125 - real_acc: 0.7500 - fake_acc - ETA: 1:20 - g_loss: 1.1593 - d_loss: 0.9787 - acc: 0.7969 - real_acc: 0.7500 - fake_acc: 0.84 - ETA: 1:20 - g_loss: 1.1724 - d_loss: 0.8488 - acc: 0.7969 - real_acc: 0.8438 - fake_acc:  - ETA: 1:20 - g_loss: 1.0985 - d_loss: 1.1282 - acc: 0.7344 - real_acc: 0.6562 - fake_acc: 0.81 - ETA: 1:20 - g_loss: 1.1758 - d_loss: 1.0751 - acc: 0.6562 - real_acc: 0.6250 - fake - ETA: 1:19 - g_loss: 1.3145 - d_loss: 0.9550 - acc: 0.7812 - real_acc: 0.7812 - fake_acc: 0.78 - ETA: 1:19 - g_loss: 1.3363 - d_loss: 0.8240 - acc: 0.8594 - real_acc: 0.8125 - fake_acc: 0. - ETA: 1:19 - g_loss: 1.2732 - d_loss: 0.9369 - acc: 0.7500 - real_acc: 0.6875 - fake_acc: 0. - ETA: 1:19 - g_loss: 1.3695 - d_loss: 0.7772 - acc: 0.8438 - real_acc: 0.8438 - fake - ETA: 1:18 - g_loss: 1.2140 - d_loss: 0.8063 - acc: 0.8594 - real_acc: 0.8750 - fake_acc: 0. - ETA: 1:18 - g_loss: 1.2341 - d_loss: 0.7530 - acc: 0.8125 - real_acc: 0.8438 - fake_acc: 0.78 - ETA: 1:18 - g_loss: 1.5169 - d_loss: 0.7330 - acc: 0.8281 - real_acc: 0.7188 - fake_acc: 0.93 - ETA: 1:18 - g_loss: 1.3141 - d_loss: 0.8803 - acc: 0.8125 - real_acc: 0.7500 - fake_acc: 0.87 - ETA: 1:18 - g_loss: 1.0565 - d_loss: 0.9497 - acc: 0.7031 - real_acc: 0.8125 - fake_acc:  - ETA: 1:17 - g_loss: 1.5747 - d_loss: 0.8950 - acc: 0.7969 - real_acc: 0.7812 - fake - ETA: 1:17 - g_loss: 1.7937 - d_loss: 0.7783 - acc: 0.7969 - real_acc: 0.7812 - fake_acc:  - ETA: 1:17 - g_loss: 1.4568 - d_loss: 0.8201 - acc: 0.8125 - real_acc: 0.8438 - fake_acc: 0.78 - ETA: 1:17 - g_loss: 1.1557 - d_loss: 1.1406 - acc: 0.6875 - real_acc: 0.7188 - fake_acc: 0.65 - ETA: 1:17 - g_loss: 1.1910 - d_loss: 1.0183 - acc: 0.7188 - real_acc: 0.812 - ETA: 1:16 - g_loss: 1.9151 - d_loss: 0.3962 - acc: 0.9375 - real_acc: 0.9375 - fake_acc: 0.93 - ETA: 1:16 - g_loss: 1.6775 - d_loss: 0.7725 - acc: 0.8594 - real_acc: 0.9062 - fake_acc: 0. - ETA: 1:15 - g_loss: 1.6035 - d_loss: 0.5646 - acc: 0.8906 - real_acc: 0.9062 - fake_acc: 0.87 - ETA: 1:15 - g_loss: 2.0321 - d_loss: 0.3597 - acc: 0.9375 - real_acc: 0.9375 - fake_acc: 0.93 - ETA: 1:15 - g_loss: 1.8017 - d_loss: 0.4386 - acc: 0.9531 - real_acc: 0.9688 - fake - ETA: 1:15 - g_loss: 1.5671 - d_loss: 0.8360 - acc: 0.7969 - real_acc: 0.8438  - ETA: 1:14 - g_loss: 1.1078 - d_loss: 1.9493 - acc: 0.5469 - real_acc: 0.4688 - fake_acc: 0. - ETA: 1:14 - g_loss: 1.0718 - d_loss: 1.5336 - \n",
      "\n",
      "epoch 2/100\n",
      "1875/1875 [==============================] - ETA: 0s - g_loss: 1.4433 - d_loss: 1.8242 - acc: 0.6250 - real_acc: 0.2812 - fake_acc: 0.96 - 272s 145ms/step - g_loss: 1.4433 - d_loss: 1.8242 - acc: 0.6250 - real_acc: 0.2812 - fake_acc: 0.9688\n",
      "\n",
      "epoch 3/100\n",
      "1875/1875 [==============================] - 236s 126ms/step - g_loss: 0.6866 - d_loss: 1.3787 - acc: 0.5938 - real_acc: 0.7812 - fake_acc: 0.4062\n",
      "\n",
      "epoch 4/100\n",
      "1875/1875 [==============================] - 254s 135ms/step - g_loss: 0.8061 - d_loss: 1.1214 - acc: 0.6562 - real_acc: 0.4062 - fake_acc: 0.9062\n",
      "\n",
      "epoch 5/100\n",
      "1875/1875 [==============================] - 235s 125ms/step - g_loss: 1.2327 - d_loss: 1.1511 - acc: 0.5469 - real_acc: 0.0938 - fake_acc: 1.0000\n",
      "\n",
      "epoch 6/100\n",
      "1875/1875 [==============================] - 237s 127ms/step - g_loss: 0.6652 - d_loss: 1.4026 - acc: 0.4844 - real_acc: 0.4688 - fake_acc: 0.5000\n",
      "\n",
      "epoch 7/100\n",
      "1875/1875 [==============================] - 253s 135ms/step - g_loss: 0.6907 - d_loss: 1.3862 - acc: 0.5312 - real_acc: 0.6562 - fake_acc: 0.4062\n",
      "\n",
      "epoch 8/100\n",
      "1875/1875 [==============================] - 238s 127ms/step - g_loss: 0.6739 - d_loss: 1.3810 - acc: 0.4688 - real_acc: 0.9062 - fake_acc: 0.0312\n",
      "\n",
      "epoch 9/100\n",
      "1875/1875 [==============================] - 237s 126ms/step - g_loss: 0.6913 - d_loss: 1.3897 - acc: 0.5469 - real_acc: 0.8750 - fake_acc: 0.2188\n",
      "\n",
      "epoch 10/100\n",
      "1875/1875 [==============================] - 237s 126ms/step - g_loss: 0.6918 - d_loss: 1.3843 - acc: 0.5625 - real_acc: 0.7812 - fake_acc: 0.3438\n",
      "\n",
      "epoch 11/100\n",
      "1875/1875 [==============================] - 238s 127ms/step - g_loss: 0.6908 - d_loss: 1.3865 - acc: 0.4531 - real_acc: 0.7188 - fake_acc: 0.1875\n",
      "\n",
      "epoch 12/100\n",
      "1875/1875 [==============================] - 242s 129ms/step - g_loss: 0.6933 - d_loss: 1.3698 - acc: 0.5156 - real_acc: 0.5312 - fake_acc: 0.5000\n",
      "\n",
      "epoch 13/100\n",
      " 655/1875 [=========>....................] - ETA: 2:37 - g_loss: 0.7229 - d_loss: 1.3832 - acc: 0.5156 - real_acc: 0.0312 - fake_acc: 1.0000    "
     ]
    }
   ],
   "source": [
    "record_sample(generator, 0, show=False)\n",
    "steps_per_epoch = train_images.shape[0] // BATCH_SIZE\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'\\nepoch {epoch}/{EPOCHS}')\n",
    "    progress_bar = keras.utils.Progbar(steps_per_epoch, stateful_metrics=metrics_names)\n",
    "\n",
    "    for i, image_batch in enumerate(train_ds):\n",
    "\n",
    "        num_samples = image_batch.shape[0]\n",
    "        noise = tf.random.normal([num_samples, NOISE_DIMENSION])\n",
    "\n",
    "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "            # Train G on noise\n",
    "            generated_images = generator(noise, training=True)\n",
    "            # Train D on training images\n",
    "            trained_output = discriminator(image_batch, training=True)\n",
    "            # Train D on generated images\n",
    "            generated_output = discriminator(generated_images, training=True)\n",
    "\n",
    "            # Calculate loss\n",
    "            g_loss = generator.loss(generated_output)\n",
    "            d_loss = discriminator.loss(trained_output, generated_output)\n",
    "\n",
    "        real_acc, fake_acc = accuracy(trained_output, generated_output)\n",
    "        acc = 0.5 * (real_acc + fake_acc)\n",
    "\n",
    "        training_metrics = {\n",
    "            'g_loss': g_loss,\n",
    "            'd_loss': d_loss,\n",
    "            'acc': acc,\n",
    "            'real_acc': real_acc,\n",
    "            'fake_acc': fake_acc,\n",
    "        }\n",
    "\n",
    "        # Record loss history\n",
    "        for metric in batch_history:\n",
    "            batch_history[metric].append(training_metrics[metric])\n",
    "    \n",
    "        metric_values = training_metrics.items()\n",
    "        progress_bar.update(i, values=metric_values)        \n",
    "\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/GradientTape#gradient\n",
    "        grad_g = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "        grad_d = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#apply_gradients\n",
    "        generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(grad_d, discriminator.trainable_variables))\n",
    "\n",
    "    progress_bar.update(steps_per_epoch, values=metric_values, finalize=True)\n",
    "    record_sample(generator, epoch, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(batch_history['g_loss'], label='Generator Loss')\n",
    "plt.plot(batch_history['d_loss'], label='Discriminator Loss')\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(METRICS_PATH, 'loss.png'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(batch_history['acc'], label='Total Accuracy')\n",
    "plt.plot(batch_history['real_acc'], label='Training Data Accuracy')\n",
    "plt.plot(batch_history['fake_acc'], label='Generated Data Accuracy')\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(METRICS_PATH, 'accuracy.png'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test_preds = tf.dtypes.cast(tf.math.round(discriminator(test_images, training=False)), tf.float32)\n",
    "fake_test_preds = tf.dtypes.cast(tf.math.round(discriminator(generator(tf.random.normal([test_images.shape[0], NOISE_DIMENSION]), training=False), training=False)), tf.float32)\n",
    "real_labels = tf.ones_like(real_test_preds)\n",
    "fake_labels = tf.zeros_like(fake_test_preds)\n",
    "preds = tf.concat([real_test_preds, fake_test_preds], axis=0)\n",
    "labels = tf.concat([real_labels, fake_labels], axis=0)\n",
    "preds = tf.squeeze(preds)\n",
    "labels = tf.squeeze(labels)\n",
    "named_labels = ['Fake', 'Real']\n",
    "confusion_matrix = tf.math.confusion_matrix(labels, preds, num_classes=2).numpy()\n",
    "plt.figure(figsize=(10, 7))\n",
    "sn.set(font_scale=1.4)\n",
    "ax = sn.heatmap(confusion_matrix, annot=True, annot_kws={'size': 16}, fmt='d', cmap=sn.cm.rocket_r, xticklabels=named_labels, yticklabels=named_labels)\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top')\n",
    "ax.tick_params(axis='both', which='major', labelsize=16, labelbottom=False, bottom=False, top=False, labeltop=True)\n",
    "plt.savefig(os.path.join(METRICS_PATH, 'confusion_matrix.png'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with imageio.get_writer(os.path.join(METRICS_PATH, 'gan.gif'), mode='I') as writer:\n",
    "    filenames = glob.glob(os.path.join(OUTPUT_PATH, 'epoch*.png'))\n",
    "    filenames = sorted(filenames)\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "Image.open(os.path.join(OUTPUT_PATH, 'epoch_{:04d}.png'.format(epoch)))\n",
    "\n",
    "# for _ in range(5):\n",
    "#     real_test_image = test_images[tf.random.uniform(shape=(), minval=0, maxval=TEST_SIZE+1, dtype=tf.int32)]\n",
    "#     fake_test_image_1 = generator(tf.random.normal([1, NOISE_DIMENSION]), training=False)\n",
    "#     fake_test_image_2 = tf.nn.tanh(tf.random.normal(train_images.shape[1:]))\n",
    "\n",
    "#     ax1 = plt.subplot(1, 3, 1)\n",
    "#     ax1.imshow(real_test_image, cmap=plt.cm.gray)\n",
    "#     ax1.axis('off')\n",
    "#     real_test_image = tf.expand_dims(input=real_test_image, axis=0)\n",
    "#     ax1.set_title(discriminator(real_test_image, training=False).numpy()[0][0])\n",
    "\n",
    "#     ax2 = plt.subplot(1, 3, 2)\n",
    "#     ax2.imshow(fake_test_image_1[0, :, :, 0], cmap=plt.cm.gray)\n",
    "#     ax2.axis('off')\n",
    "#     ax2.set_title(discriminator(fake_test_image_1, training=False).numpy()[0][0])\n",
    "\n",
    "#     ax3 = plt.subplot(1, 3, 3)\n",
    "#     ax3.imshow(fake_test_image_2, cmap=plt.cm.gray)\n",
    "#     ax3.axis('off')\n",
    "#     fake_test_image_2 = tf.expand_dims(input=fake_test_image_2, axis=0)\n",
    "#     ax3.set_title(discriminator(fake_test_image_1, training=False).numpy()[0][0])\n",
    "\n",
    "#     plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan-vae",
   "language": "python",
   "name": "gan-vae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
