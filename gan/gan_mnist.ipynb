{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Generation using GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('Running TF without GPU')\n",
    "else:\n",
    "    print(f'Found GPU at {device_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import math\n",
    "import os\n",
    "import seaborn as sn\n",
    "import time\n",
    "\n",
    "from abc import abstractstaticmethod\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "\n",
    "sn.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_DIMENSION = 100\n",
    "TRAIN_SIZE = 60000\n",
    "TEST_SIZE = 10000\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "\n",
    "EPSILON = 1e-7\n",
    "N_EXAMPLES = 25\n",
    "G_LEARNING_RATE = 2e-4\n",
    "D_LEARNING_RATE = 2e-4\n",
    "\n",
    "ARCH = 'gan'\n",
    "path_prefix = ''\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    path_prefix = '/content/drive/My Drive/gan-vae/gan/'\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    \n",
    "METRICS_PATH = f'{path_prefix}metrics/{ARCH}/'\n",
    "OUTPUT_PATH = f'{path_prefix}output/{ARCH}/'\n",
    "\n",
    "if not IN_COLAB:\n",
    "    !mkdir -p $METRICS_PATH\n",
    "    !mkdir -p $OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert NOISE_DIMENSION > 0\n",
    "assert TRAIN_SIZE <= 600000\n",
    "assert TEST_SIZE <= 10000\n",
    "assert BATCH_SIZE >= 1\n",
    "assert EPOCHS >= 1\n",
    "assert N_EXAMPLES >= 1\n",
    "assert G_LEARNING_RATE > 0\n",
    "assert D_LEARNING_RATE > 0\n",
    "assert EPSILON > 0\n",
    "assert N_EXAMPLES <= TEST_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNetwork(tf.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, input_data, training=False) -> tf.Tensor:\n",
    "        output_data = input_data\n",
    "        for layer in self.layers:\n",
    "            output_data = layer(output_data, training=training)\n",
    "        return output_data\n",
    "\n",
    "    @abstractstaticmethod\n",
    "    def loss() -> tf.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractstaticmethod\n",
    "    def optimizer(*args, **kwargs) -> tf.optimizers.Optimizer:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Network\n",
    "### Layers\n",
    "<pre>\n",
    "<b>Input</b>:  input_size=(128, 1)\n",
    "<b>Dense</b>:  units=128, activation=ReLU\n",
    "<b>Dense</b>:  units=256, activation=ReLU\n",
    "<b>Dense</b>:  units=512, activation=ReLU\n",
    "<b>Dense</b>:  units=784, activation=tanh, target_shape=(28, 28, 1)\n",
    "</pre>\n",
    "\n",
    "### Optimizer\n",
    "<pre>\n",
    "<b>Adam</b>:  learning_rate=0.0002\n",
    "</pre>\n",
    "\n",
    "### Loss\n",
    "&#8466;<sub>G</sub>(<i><b>z</b></i>) = <sup>-1</sup>&frasl;<sub>m</sub> &lowast; &sum;<sub><i>i</i></sub> log <i>D</i>(G(<i><b>z</b><sup>(i)</sup></i>))\n",
    "\n",
    "### Goal\n",
    "Find argmin<sub>G</sub> {&#8466;<sub>G</sub>(<i><b>z</b></i>)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(BaseNetwork):\n",
    "    def __init__(self, noise_dimension: int, output_shape: tuple) -> None:\n",
    "        super().__init__()\n",
    "        # Network layers\n",
    "        self.layers = [\n",
    "            keras.layers.InputLayer(input_shape=(noise_dimension,)),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(units=128),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=256),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=512),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=tf.reduce_prod(output_shape)),\n",
    "            keras.layers.Reshape(target_shape=output_shape),\n",
    "            keras.layers.Activation(tf.nn.tanh),\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def optimizer(learning_rate: float) -> tf.optimizers.Optimizer:\n",
    "        return tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    @tf.function\n",
    "    def loss(generated_output):\n",
    "        loss_i = tf.math.log(generated_output+EPSILON)\n",
    "        loss = -tf.math.reduce_mean(loss_i)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network\n",
    "### Layers\n",
    "<pre>\n",
    "<b>Input</b>:    input_size=(28, 28, 1)\n",
    "<b>Dense</b>:    units=512, activation=ReLU\n",
    "<b>Dropout</b>:  rate=0.4\n",
    "<b>Dense</b>:    units=512, activation=ReLU\n",
    "<b>Dropout</b>:  rate=0.3\n",
    "<b>Dense</b>:    units=512, activation=ReLU\n",
    "<b>Dropout</b>:  rate=0.3\n",
    "<b>Dense</b>:    units=256, activation=ReLU\n",
    "<b>Dropout</b>:  rate=0.2\n",
    "<b>Dense</b>:    units=128, activation=ReLU\n",
    "<b>Dropout</b>:  rate=0.1\n",
    "<b>Dense</b>:    units=64, activation=ReLU\n",
    "<b>Dense</b>:    units=1, activation=sigmoid\n",
    "</pre>\n",
    "\n",
    "### Optimizer\n",
    "<pre>\n",
    "<b>Adam</b>:         learning_rate=0.0002\n",
    "</pre>\n",
    "\n",
    "### Loss\n",
    "&#8466;<sub>D</sub>(<i><b>x</b>,<b>z</b></i>) = <sup>-1</sup>&frasl;<sub>m</sub> &lowast; &sum;<sub><i>i</i></sub> \\[log <i>D</i>(<i><b>x</b><sup>(i)</sup></i>) + log (1-<i>D</i>(G(<i><b>z</b><sup>(i)</sup></i>)))\\]\n",
    "\n",
    "### Goal\n",
    "Find argmin<sub>D</sub> &#8466;<sub>D</sub>(<i><b>x</b></i>,<i><b>z</b></i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(BaseNetwork):\n",
    "    def __init__(self, input_shape: tuple) -> None:\n",
    "        super().__init__()\n",
    "        # Network layers\n",
    "        self.layers = [\n",
    "            keras.layers.InputLayer(input_shape=input_shape),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(units=512),\n",
    "            keras.layers.Dropout(rate=0.3),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=512),\n",
    "            keras.layers.Dropout(rate=0.3),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=256),\n",
    "            keras.layers.Dropout(rate=0.2),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=128),\n",
    "            keras.layers.ReLU(),\n",
    "            keras.layers.Dense(units=1),\n",
    "            keras.layers.Activation(tf.nn.sigmoid),\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def optimizer(learning_rate: float):\n",
    "        return tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    @tf.function\n",
    "    def loss(trained_ouput, generated_output) -> tf.Tensor:\n",
    "        loss_i = tf.math.log(trained_ouput+EPSILON) + tf.math.log1p(-generated_output+EPSILON)\n",
    "        loss = -tf.math.reduce_mean(loss_i)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* Import MNIST training images\n",
    "* Normalize to \\[-1, 1\\]\n",
    "* Shuffle and batch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, _), (test_images, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = tf.dtypes.cast((train_images[:TRAIN_SIZE]-127.5) / 127.5, tf.float32)\n",
    "train_images = tf.expand_dims(input=train_images, axis=-1)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_images) \\\n",
    "                          .shuffle(TRAIN_SIZE) \\\n",
    "                          .batch(BATCH_SIZE)\n",
    "\n",
    "test_images = tf.dtypes.cast((train_images[:TEST_SIZE]-127.5) / 127.5, tf.float32)\n",
    "test_images = tf.expand_dims(input=test_images, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(noise_dimension=NOISE_DIMENSION, output_shape=(train_images.shape[1:]))\n",
    "generator_optimizer = generator.optimizer(G_LEARNING_RATE)\n",
    "\n",
    "discriminator = Discriminator(input_shape=(train_images.shape[1:]))\n",
    "discriminator_optimizer = discriminator.optimizer(D_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = tf.random.normal([N_EXAMPLES, NOISE_DIMENSION])\n",
    "metric_names = ['g_loss', 'd_loss', 'acc', 'real_acc', 'fake_acc']\n",
    "batch_history = { name: [] for name in metric_names }\n",
    "epoch_history = { name: [] for name in metric_names[2:] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_sample(generator: Generator, epoch: int, save: bool=True, show=True):\n",
    "    fixed_predictions = generator(fixed_noise)\n",
    "    random_predictions = generator(tf.random.normal(fixed_noise.shape))\n",
    "\n",
    "    titles = ['Fixed Noise Samples', 'Variational Noise Samples']\n",
    "    data = [fixed_predictions, random_predictions]\n",
    "\n",
    "    f = plt.figure(figsize=(8, 4))\n",
    "    f.suptitle('GAN Output')\n",
    "    outer = gridspec.GridSpec(1, 2)\n",
    "\n",
    "    for i in range(2):\n",
    "        inner = gridspec.GridSpecFromSubplotSpec(int(math.sqrt(N_EXAMPLES)), int(math.sqrt(N_EXAMPLES)), subplot_spec=outer[i])\n",
    "        predictions = data[i]\n",
    "        for j in range(predictions.shape[0]):\n",
    "            ax = plt.Subplot(f, inner[j])\n",
    "            ax.imshow(predictions[j][:, :, 0]*127.5 + 127.5, cmap=plt.cm.gray)\n",
    "            ax.axis('off')\n",
    "            if j == int(math.sqrt(N_EXAMPLES)) // 2:\n",
    "                ax.set_title(titles[i])\n",
    "            f.add_subplot(ax)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(OUTPUT_PATH, 'epoch_{:04d}.png'.format(epoch)))\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_accuracy(real_output) -> tf.Tensor:\n",
    "    # Trained images fed into D have output 1 \n",
    "    real_acc = tf.math.reduce_mean(tf.dtypes.cast(tf.math.equal(tf.math.round(real_output), 1.), tf.float32))\n",
    "    return real_acc\n",
    "\n",
    "def accuracy(real_output, fake_output) -> tuple:\n",
    "    # Images from G's noisy distribution should have output 0 from D\n",
    "    correct_fake_output = tf.zeros_like(fake_output)\n",
    "    fake_acc = tf.math.reduce_mean(tf.dtypes.cast(tf.math.equal(tf.math.round(fake_output), 0.), tf.float32))\n",
    "\n",
    "    return real_accuracy(real_output), fake_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Algorithm\n",
    "<pre>\n",
    "<b>for</b> number of training iterations <b>do</b>\n",
    "  <b>for</b> k steps <b>do</b>\n",
    "     • Sample minibatch of <i>m</i> noise samples {<i><b>z</b><sup>(1)</sup>, ..., <b>z</b><sup>(m)</sup></i>} from noise prior <i>p<sub>g</sub>(<b>z</b>)</i>.\n",
    "     • Sample minibatch of <i>m</i> examples {<i><b>x</b><sup>(1)</sup>, ..., <b>x</b><sup>(m)</sup></i>} from data generating distribution <i>p<sub>data</sub>(<b>x</b>)</i>.\n",
    "     • Update the discriminator by <u>ascending</u> its stochastic gradient:\n",
    "       <center>&Del;<sub>&theta;<sub>d</sub></sub> <sup>1</sup>&frasl;<sub>m</sub> &lowast; &sum;<sub><i>i</i></sub> [log <i>D</i>(<i><b>x</b><sup>(i)</sup></i>) + log(1-<i>D</i>(G(<i><b>z</b><sup>(i)</sup></i>)))]</center>\n",
    "  <b>end for</b>\n",
    "  • Sample minibatch of <i>m</i> noise samples {<i><b>z</b><sup>(1)</sup>, ..., <b>z</b><sup>(m)</sup></i>} from noise prior <i>p<sub>g</sub>(<b>z</b>)</i>.\n",
    "  • Update the generator by <u>descending</u> its stochastic gradient:\n",
    "  <center>&Del;<sub>&theta;<sub>d</sub></sub> <sup>1</sup>&frasl;<sub>m</sub> &lowast; &sum;<sub><i>i</i></sub> log(1-<i>D</i>(G(<i><b>z</b><sup>(i)</sup></i>)))</center>\n",
    "<b>end for</b>\n",
    "</pre>\n",
    "\n",
    "Source: https://arxiv.org/pdf/1406.2661.pdf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "record_sample(generator, 0, show=False)\n",
    "steps_per_epoch = train_images.shape[0] // BATCH_SIZE\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'\\nepoch {epoch}/{EPOCHS}')\n",
    "    progress_bar = keras.utils.Progbar(steps_per_epoch, stateful_metrics=metric_names)\n",
    "\n",
    "    for i, image_batch in enumerate(train_ds):\n",
    "\n",
    "        num_samples = image_batch.shape[0]\n",
    "        noise = tf.random.normal([num_samples, NOISE_DIMENSION])\n",
    "\n",
    "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "            # Train G on noise\n",
    "            generated_images = generator(noise, training=True)\n",
    "            # Train D on training images\n",
    "            trained_output = discriminator(image_batch, training=True)\n",
    "            # Train D on generated images\n",
    "            generated_output = discriminator(generated_images, training=True)\n",
    "\n",
    "            # Calculate loss\n",
    "            g_loss = generator.loss(generated_output)\n",
    "            d_loss = discriminator.loss(trained_output, generated_output)\n",
    "\n",
    "        real_acc, fake_acc = accuracy(trained_output, generated_output)\n",
    "        acc = 0.5 * (real_acc + fake_acc)\n",
    "\n",
    "        batch_metrics = {\n",
    "            'g_loss': g_loss,\n",
    "            'd_loss': d_loss,\n",
    "            'acc': acc,\n",
    "            'real_acc': real_acc,\n",
    "            'fake_acc': fake_acc,\n",
    "        }\n",
    "\n",
    "        # Record loss history\n",
    "        for metric in batch_history:\n",
    "            batch_history[metric].append(batch_metrics[metric])\n",
    "    \n",
    "        batch_metric_values = batch_metrics.items()\n",
    "        progress_bar.update(i, values=batch_metric_values)        \n",
    "\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/GradientTape#gradient\n",
    "        grad_g = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "        grad_d = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#apply_gradients\n",
    "        generator_optimizer.apply_gradients(zip(grad_g, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(grad_d, discriminator.trainable_variables))\n",
    "\n",
    "    for metric in epoch_history:\n",
    "        epoch_history[metric].append(batch_history[metric][-1])\n",
    "\n",
    "    record_sample(generator, epoch, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(tf.math.reduce_mean(tf.reshape(batch_history['g_loss'], shape=(-1, math.ceil(TRAIN_SIZE / BATCH_SIZE))), axis=1), label='Generator Loss')\n",
    "plt.plot(tf.math.reduce_mean(tf.reshape(batch_history['d_loss'], shape=(-1, math.ceil(TRAIN_SIZE / BATCH_SIZE))), axis=1), label='Discriminator Loss')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GAN Loss')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(METRICS_PATH, 'loss.png'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(tf.math.reduce_mean(tf.reshape(batch_history['acc'], shape=(-1, math.ceil(TRAIN_SIZE / BATCH_SIZE))), axis=1), label='Total Accuracy')\n",
    "plt.plot(tf.math.reduce_mean(tf.reshape(batch_history['real_acc'], shape=(-1, math.ceil(TRAIN_SIZE / BATCH_SIZE))), axis=1), label='Training Data Accuracy')\n",
    "plt.plot(tf.math.reduce_mean(tf.reshape(batch_history['fake_acc'], shape=(-1, math.ceil(TRAIN_SIZE / BATCH_SIZE))), axis=1), label='Generated Data Accuracy')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('GAN Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(METRICS_PATH, 'accuracy.png'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with imageio.get_writer(os.path.join(METRICS_PATH, f'{ARCH}.gif'), mode='I') as writer:\n",
    "    filenames = glob.glob(os.path.join(OUTPUT_PATH, 'epoch*.png'))\n",
    "    filenames = sorted(filenames)\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "\n",
    "last_epoch_img = 'epoch_{:04d}.png'.format(EPOCHS)\n",
    "last_epoch_img_output = os.path.join(OUTPUT_PATH, last_epoch_img)\n",
    "last_epoch_img_metrics = os.path.join(METRICS_PATH, last_epoch_img)\n",
    "Image.open(last_epoch_img_output).save(last_epoch_img_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan-vae",
   "language": "python",
   "name": "gan-vae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
