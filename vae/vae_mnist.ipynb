{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Recognition using VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('Running TF without GPU')\n",
    "else:\n",
    "    print(f'Found GPU at {device_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import math\n",
    "import os\n",
    "import seaborn as sn\n",
    "import time\n",
    "\n",
    "from abc import abstractstaticmethod\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "\n",
    "sn.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 60000\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200\n",
    "\n",
    "N_EXAMPLES = 25\n",
    "LEARNING_RATE = 1e-4\n",
    "LATENT_DIMENSION = 3\n",
    "\n",
    "ARCH = 'vae'\n",
    "METRICS_PATH = f'metrics/{ARCH}/'\n",
    "OUTPUT_PATH = f'output/{ARCH}/'\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    path_prefix = '/content/drive/My Drive/gan-vae/vae/'\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    \n",
    "METRICS_PATH = f'{path_prefix}metrics/{ARCH}/'\n",
    "OUTPUT_PATH = f'{path_prefix}output/{ARCH}/'\n",
    "\n",
    "if not IN_COLAB:\n",
    "    !mkdir -p $METRICS_PATH\n",
    "    !mkdir -p $OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNetwork(tf.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, input_data, training=False) -> tf.Tensor:\n",
    "        output_data = input_data\n",
    "        for layer in self.layers:\n",
    "            output_data = layer(output_data, training=training)\n",
    "        return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Network\n",
    "### Layers\n",
    "<pre>\n",
    "<b>Input</b>:  input_size=(28, 28, 1), target_shape=(784, 1)\n",
    "<b>Dense</b>:  units=256, activation=tanh\n",
    "<b>Dense</b>:  units=2*2, target_shape=(3, 2)\n",
    "</pre>\n",
    "### Output\n",
    "The output layer of the encoder network represents the mean (&mu;) and log-variance (log &sigma;<sup>2</sup>) of the posterior distribution which the decoder should sample from. The output looks as follows:\n",
    "<pre>\n",
    "         [ &mu;<sub>1</sub> &sigma;<sub>1</sub> ]\n",
    "e(x)  ~  [ &mu;<sub>2</sub> &sigma;<sub>2</sub> ]\n",
    "         [ &mu;<sub>3</sub> &sigma;<sub>3</sub> ]\n",
    "</pre>\n",
    "Using the reparameterization trick, the input to the decoder network becomes:\n",
    "<center><i><b>z</b></i> = &mu; + &sigma; &xodot; &epsilon;</center>\n",
    "where &epsilon; ~ <i>N</i>(0,<i>I</i>). We use log &sigma;<sup>2</sup> here since the parameters the network outputs does not have a bound just as log &sigma;<sup>2</sup> has no bound on its range, i.e the output does not have any non-linearity which would normally restrict the range of the output values, as to not completely restrict the latent distribution which the values parameterize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(BaseNetwork):\n",
    "    def __init__(self, input_shape: tuple, latent_dimension: int) -> None:\n",
    "        super().__init__()\n",
    "        self.latent_dimension = latent_dimension\n",
    "        self.layers = [\n",
    "            keras.layers.InputLayer(input_shape=input_shape),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(units=256),\n",
    "            keras.layers.Activation(tf.nn.tanh),\n",
    "            keras.layers.Dense(units=latent_dimension*2),\n",
    "            keras.layers.Reshape(target_shape=(latent_dimension, 2))\n",
    "        ]\n",
    "    \n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, input_data, training=False) -> tf.Tensor:\n",
    "        output_data = input_data\n",
    "        for layer in self.layers:\n",
    "            output_data = layer(output_data, training=training)\n",
    "        mean, log_var = output_data[:, :, 0], output_data[:, :, 1]\n",
    "        epsilon = tf.random.normal(shape=log_var.shape)\n",
    "        # sd = e^(log(var^.5)) = e^(0.5*log(var))\n",
    "        stddev = tf.math.multiply(epsilon, tf.math.exp(0.5 * log_var))\n",
    "        # Reparameterization: z ~ N(mean, stddev * epsilon)\n",
    "        return mean + stddev*epsilon, mean, tf.math.exp(log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Network\n",
    "### Layers\n",
    "<pre>\n",
    "<b>Input</b>:   input_size=(2,)\n",
    "<b>Dense</b>:   units=256, activation=tanh\n",
    "<b>Dense</b>:   units=784, activation=sigmoid, target_shape=(28, 28)\n",
    "</pre>\n",
    "\n",
    "### Optimizer\n",
    "<pre>\n",
    "<b>Adagrad</b>: learning_rate=0.0001\n",
    "</pre>\n",
    "\n",
    "### Loss\n",
    "<pre>\n",
    "<b>&Lscr;</b>(<i>&theta;</i>,<i>&phi;</i>) = -<i>D<sub>KL</sub></i>(<i>q<sub>&phi;</sub></i>(<i><b>z</b></i>) &vert;&vert; <i>p<sub>&theta;</sub></i>(<i><b>z</b></i>)) + &Eopf;<sub><i>q<sub>&phi;</sub></i>(<i><b>z</b></i>&vert;<i><b>x</b></i>)</sub>log <i>p<sub>&theta;</sub></i>(<i><b>x</b></i>&vert;<i><b>z</b></i>)\n",
    "        &approx; &frac12; &Sum;<sub>j</sub> [1 + log(<i>&sigma;<sub>j</sub><sup>2</sup></i>) - <i>&mu;<sub>j</sub><sup>2</sup></i> - <i>&sigma;<sub>j</sub><sup>2</sup></i>] + &Eopf;<sub><i>q<sub>&phi;</sub></i>(<i><b>z</b></i>&vert;<i><b>x</b></i>)</sub>log <i>p<sub>&theta;</sub></i>(<i><b>x</b></i>&vert;<i><b>z</b></i>)\n",
    "        &approx; &frac12; &Sum;<sub>j</sub> [1 + log(<i>&sigma;<sub>j</sub><sup>2</sup></i>) - <i>&mu;<sub>j</sub><sup>2</sup></i> - <i>&sigma;<sub>j</sub><sup>2</sup></i>] +  &Sum;<sub>i</sub> [<i>x<sub>i</sub></i>log(<i>y<sub>i</sub></i>) + (1 - <i>x<sub>i</sub></i>)&lowast;log(1 - <i>y<sub>i</sub></i>)]\n",
    "</pre>\n",
    "\n",
    "### Goal\n",
    "Find argmin<sub><i>&theta;</i>,<i>&phi;</i></sub> <b>&Lscr;</b>(<i>&theta;</i>,<i>&phi;</i>), and use <i>p<sub>&theta;</sub></i>(<i><b>x</b></i>&vert;<i><b>z</b></i>) as a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(BaseNetwork):\n",
    "    def __init__(self, latent_dimension: int, output_shape: tuple) -> None:\n",
    "        super().__init__()\n",
    "        self.latent_dimension = latent_dimension\n",
    "        self.layers = [\n",
    "            keras.layers.InputLayer(input_shape=(latent_dimension,)),\n",
    "            keras.layers.Dense(units=256),\n",
    "            keras.layers.Activation(tf.nn.tanh),\n",
    "            keras.layers.Dense(units=tf.math.reduce_prod(output_shape)),\n",
    "            keras.layers.Activation(tf.nn.sigmoid),\n",
    "            keras.layers.Reshape(target_shape=output_shape),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* Import MNIST images\n",
    "* Normalize to \\[0, 1\\]\n",
    "* Collapse pixels to 0/1 for sharper image features\n",
    "* Shuffle and batch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, _), (test_images, _) = keras.datasets.mnist.load_data()\n",
    "train_images = train_images[:TRAIN_SIZE] / 255.\n",
    "train_images = tf.round(train_images)\n",
    "train_images = tf.dtypes.cast(train_images, tf.float32)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images) \\\n",
    "                               .shuffle(TRAIN_SIZE) \\\n",
    "                               .batch(BATCH_SIZE)\n",
    "image_shape = train_images.shape[1:]\n",
    "\n",
    "test_images = test_images[:N_EXAMPLES] / 255.\n",
    "test_images = tf.round(test_images)\n",
    "test_images = tf.dtypes.cast(test_images, tf.float32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_images) \\\n",
    "                              .shuffle(TRAIN_SIZE) \\\n",
    "                              .batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(image_shape, LATENT_DIMENSION)\n",
    "decoder = Decoder(LATENT_DIMENSION, image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define metrics to record through training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoded_tests = test_images[:N_EXAMPLES, :, :]\n",
    "metric_names = ['kl_loss', 'rec_loss', 'loss']\n",
    "batch_history = { name: [] for name in metric_names }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture training visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_sample(encoder: Encoder, decoder: Decoder, epoch: int, save: bool=True, show: bool=True):\n",
    "    encoded_input, _, _ = encoder(sample_encoded_tests)\n",
    "    sampled_input = tf.random.normal(encoded_input.shape)\n",
    "\n",
    "    encoder_decoder = decoder(encoded_input)\n",
    "    sampled_decoder = decoder(sampled_input)\n",
    "\n",
    "    f = plt.figure(figsize=(12, 4))\n",
    "    f.suptitle('VAE Output')\n",
    "    outer = gridspec.GridSpec(1, 3)\n",
    "    titles = ['Ground Truth', 'Encoder Latent Distribution', 'Standard Normal Distribution']\n",
    "    data = [sample_encoded_tests, encoder_decoder, sampled_decoder]\n",
    "    for i in range(3):\n",
    "        inner = gridspec.GridSpecFromSubplotSpec(int(math.sqrt(N_EXAMPLES)), int(math.sqrt(N_EXAMPLES)), subplot_spec=outer[i])\n",
    "        predictions = data[i]\n",
    "        for j in range(predictions.shape[0]):\n",
    "            ax = plt.Subplot(f, inner[j])\n",
    "            ax.imshow(predictions[j] * 255, cmap=plt.cm.gray)\n",
    "            ax.axis('off')\n",
    "            if j == int(math.sqrt(N_EXAMPLES)) // 2:\n",
    "                ax.set_title(titles[i])\n",
    "            f.add_subplot(ax)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(OUTPUT_PATH, 'epoch_{:04d}.png'.format(epoch)))\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Algorithm\n",
    "<pre>\n",
    "<i>&theta;</i>,<i>&phi;</i> &leftarrow; Initialize parameters\n",
    "<b>repeat</b>\n",
    "    <i>X<sup>M</sup></i>  &leftarrow; Random minibatch of <i>M</i> datapoints (drawn from full dataset)\n",
    "    <i>&epsilon;</i>   &leftarrow; Random samples from noise distribution <i>p</i>(<i>&epsilon;</i>)\n",
    "    <i>g</i>   &leftarrow; &Del;<sub><i>&theta;</i>,<i>&phi;</i></sub>&Lscr;<sup>M</sup>(<i>&theta;</i>,<i>&phi;</i>; <i>X<sup>M</sup></i>,<i>&epsilon;</i>) (Gradients of minibatch estimator)\n",
    "    <i>&theta;</i>,<i>&phi;</i> &leftarrow; Update parameters using gradients <i><b>g</b></i> (e.g. SGD or Adagrad)\n",
    "<b>until</b> convergence of parameters (<i>&theta;</i>,<i>&phi;</i>)\n",
    "<b>return</b> <i>&theta;</i>,<i>&phi;</i>\n",
    "</pre>\n",
    "\n",
    "Source: https://arxiv.org/pdf/1312.6114.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "record_sample(encoder, decoder, 0, show=False)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "epsilon = 1e-8\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'\\nepoch {epoch}/{EPOCHS}')\n",
    "    progress_bar = keras.utils.Progbar(TRAIN_SIZE / BATCH_SIZE, stateful_metrics=metric_names)\n",
    "\n",
    "    for i, image_batch in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            posterior_sample, mean, var = encoder(image_batch, training=True)\n",
    "\n",
    "            reconstructed_image_batch = decoder(posterior_sample,training=True)\n",
    "\n",
    "            analytic_kl_divergence = 0.5 * tf.reduce_sum(\n",
    "                1 + tf.math.log(var+epsilon) - tf.math.square(mean) - var,\n",
    "                axis=1,\n",
    "            )\n",
    "            log_likelihood = tf.reduce_sum(\n",
    "                tf.math.add(\n",
    "                    tf.math.multiply(image_batch, tf.math.log(reconstructed_image_batch+epsilon)),\n",
    "                    tf.math.multiply((1-image_batch), tf.math.log(1-reconstructed_image_batch+epsilon)),\n",
    "                ),\n",
    "                axis=[1,2],\n",
    "            )\n",
    "            kl_loss = -tf.reduce_mean(analytic_kl_divergence)\n",
    "            rec_loss = -tf.reduce_mean(log_likelihood)\n",
    "            loss = kl_loss + rec_loss\n",
    "\n",
    "        training_metrics = {\n",
    "            'kl_loss': kl_loss,\n",
    "            'rec_loss': rec_loss,\n",
    "            'loss': loss,\n",
    "        }\n",
    "\n",
    "        # Record loss history\n",
    "        for metric in batch_history:\n",
    "            batch_history[metric].append(training_metrics[metric])\n",
    "\n",
    "        metric_values = training_metrics.items()\n",
    "        progress_bar.update(i, values=metric_values)\n",
    "\n",
    "        model_vars = [*encoder.trainable_variables, *decoder.trainable_variables]\n",
    "        grad = tape.gradient(loss, model_vars)\n",
    "        optimizer.apply_gradients(zip(grad, model_vars))\n",
    "\n",
    "    record_sample(encoder, decoder, epoch, show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = gridspec.GridSpec(2, 2)\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "batch_label = 'Batch #'\n",
    "loss_label = 'Loss'\n",
    "\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "plt.title('KL Divergence')\n",
    "plt.xlabel(batch_label)\n",
    "plt.ylabel(loss_label)\n",
    "plt.plot(tf.math.reduce_mean(tf.reshape(batch_history['rec_loss'], shape=(-1, math.ceil(TRAIN_SIZE / BATCH_SIZE))), axis=1))\n",
    "\n",
    "ax = plt.subplot(gs[0, 1])\n",
    "plt.title('Reconstruction Loss')\n",
    "plt.xlabel(batch_label)\n",
    "plt.ylabel(loss_label)\n",
    "plt.plot(tf.math.reduce_mean(tf.reshape(batch_history['rec_loss'], shape=(-1, math.ceil(TRAIN_SIZE / BATCH_SIZE))), axis=1))\n",
    "\n",
    "ax = plt.subplot(gs[1, :])\n",
    "plt.title('Total Loss')\n",
    "plt.xlabel(batch_label)\n",
    "plt.ylabel(loss_label)\n",
    "plt.plot(tf.math.reduce_mean(tf.reshape(batch_history['loss'], shape=(-1, math.ceil(TRAIN_SIZE / BATCH_SIZE))), axis=1))\n",
    "\n",
    "plt.savefig(os.path.join(METRICS_PATH, 'loss.png'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with imageio.get_writer(os.path.join(METRICS_PATH, f'{ARCH}.gif'), mode='I') as writer:\n",
    "    filenames = glob.glob(os.path.join(OUTPUT_PATH, 'epoch*.png'))\n",
    "    filenames = sorted(filenames)\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "final = Image.open(os.path.join(OUTPUT_PATH, 'epoch_{:04d}.png'.format(epoch)))\n",
    "final.save(os.path.join(METRICS_PATH, 'epoch_{:04d}.png'.format(epoch)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan-vae",
   "language": "python",
   "name": "gan-vae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
